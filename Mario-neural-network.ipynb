{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f937ad",
   "metadata": {},
   "source": [
    "MARIO ENVIRONMENT SETUP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "939f4dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-super-mario-bros==7.3.0 in e:\\python39\\lib\\site-packages (7.3.0)\n",
      "Requirement already satisfied: opencv-python in e:\\python39\\lib\\site-packages (4.5.5.64)\n",
      "Requirement already satisfied: nes-py>=8.0.0 in e:\\python39\\lib\\site-packages (from gym-super-mario-bros==7.3.0) (8.1.8)\n",
      "Requirement already satisfied: numpy>=1.17.3 in e:\\python39\\lib\\site-packages (from opencv-python) (1.22.4)\n",
      "Requirement already satisfied: pyglet<=1.5.11,>=1.4.0 in e:\\python39\\lib\\site-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (1.5.11)\n",
      "Requirement already satisfied: gym>=0.17.2 in e:\\python39\\lib\\site-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.23.1)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in e:\\python39\\lib\\site-packages (from nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.64.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0 in e:\\python39\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (4.11.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in e:\\python39\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (2.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in e:\\python39\\lib\\site-packages (from gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.0.6)\n",
      "Requirement already satisfied: colorama in e:\\python39\\lib\\site-packages (from tqdm>=4.48.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in e:\\python39\\lib\\site-packages (from importlib-metadata>=4.10.0->gym>=0.17.2->nes-py>=8.0.0->gym-super-mario-bros==7.3.0) (3.8.0)\n",
      "Requirement already satisfied: scikit-image in e:\\python39\\lib\\site-packages (0.19.2)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in e:\\python39\\lib\\site-packages (from scikit-image) (2022.5.4)\n",
      "Requirement already satisfied: numpy>=1.17.0 in e:\\python39\\lib\\site-packages (from scikit-image) (1.22.4)\n",
      "Requirement already satisfied: networkx>=2.2 in e:\\python39\\lib\\site-packages (from scikit-image) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.4.1 in e:\\python39\\lib\\site-packages (from scikit-image) (1.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\python39\\lib\\site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in e:\\python39\\lib\\site-packages (from scikit-image) (9.1.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in e:\\python39\\lib\\site-packages (from scikit-image) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in e:\\python39\\lib\\site-packages (from scikit-image) (2.19.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in e:\\python39\\lib\\site-packages (from packaging>=20.0->scikit-image) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym-super-mario-bros==7.3.0 opencv-python\n",
    "!pip install scikit-image\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, numpy as np\n",
    "from skimage import transform\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "\n",
    "#NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959e11f4",
   "metadata": {},
   "source": [
    "INITIALIZE ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d2aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3')\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(\n",
    "    env,\n",
    "    [['right'],\n",
    "    ['right', 'A']]\n",
    ")\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f'{next_state.shape},\\n {reward},\\n {done},\\n {info}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77822c",
   "metadata": {},
   "source": [
    "PREPROCESS ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f9ba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        resize_obs = transform.resize(observation, self.shape)\n",
    "        # cast float back to uint8\n",
    "        resize_obs *= 255\n",
    "        resize_obs = resize_obs.astype(np.uint8)\n",
    "        return resize_obs\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df1d36",
   "metadata": {},
   "source": [
    "APPLY WRAPPERS TO ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d3e5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea9eee",
   "metadata": {},
   "source": [
    "MARIO AGENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ccc2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dcd153",
   "metadata": {},
   "source": [
    "ACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d92888d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    self.state_dim = state_dim\n",
    "    self.action_dim = action_dim\n",
    "    self.save_dir = save_dir\n",
    "\n",
    "    self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "    self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "    if self.use_cuda:\n",
    "      self.net = self.net.to(device='cuda')\n",
    "\n",
    "    self.exploration_rate = 1\n",
    "    self.exploration_rate_decay = 0.99999975\n",
    "    self.exploration_rate_min = 0.1\n",
    "    self.curr_step = 0\n",
    "\n",
    "    self.save_every = 5e5   # no. of experiences between saving Mario Net\n",
    "\n",
    "\n",
    "  def act(self, state):\n",
    "    \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "    Inputs:\n",
    "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    action_idx (int): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "    # EXPLORE\n",
    "    if np.random.rand() < self.exploration_rate:\n",
    "        action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "    # EXPLOIT\n",
    "    else:\n",
    "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "        state = state.unsqueeze(0)\n",
    "        action_values = self.net(state, model='online')\n",
    "        action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "    # decrease exploration_rate\n",
    "    self.exploration_rate *= self.exploration_rate_decay\n",
    "    self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "    # increment step\n",
    "    self.curr_step += 1\n",
    "    return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2770d1d",
   "metadata": {},
   "source": [
    "CACHE AND RECALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbf1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario): # subclassing for continuity\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.memory = deque(maxlen=10000)\n",
    "    self.batch_size = 32\n",
    "\n",
    "\n",
    "  def cache(self, state, next_state, action, reward, done):\n",
    "    \"\"\"\n",
    "    Store the experience to self.memory (replay buffer)\n",
    "    Inputs:\n",
    "    state (LazyFrame),\n",
    "    next_state (LazyFrame),\n",
    "    action (int),\n",
    "    reward (float),\n",
    "    done(bool))\n",
    "    \"\"\"\n",
    "    state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "    next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
    "    action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
    "    reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
    "    done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
    "\n",
    "    self.memory.append( (state, next_state, action, reward, done,) )\n",
    "\n",
    "\n",
    "  def recall(self):\n",
    "    \"\"\"\n",
    "    Retrieve a batch of experiences from memory\n",
    "    \"\"\"\n",
    "    batch = random.sample(self.memory, self.batch_size)\n",
    "    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56feb6e9",
   "metadata": {},
   "source": [
    "NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3302ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "  '''mini cnn structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  '''\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      c, h, w = input_dim\n",
    "\n",
    "      if h != 84:\n",
    "          raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "      if w != 84:\n",
    "          raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "      self.online = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "          nn.ReLU(),\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(3136, 512),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(512, output_dim)\n",
    "      )\n",
    "\n",
    "      self.target = copy.deepcopy(self.online)\n",
    "\n",
    "      # Q_target parameters are frozen.\n",
    "      for p in self.target.parameters():\n",
    "          p.requires_grad = False\n",
    "\n",
    "  def forward(self, input, model):\n",
    "      if model == 'online':\n",
    "          return self.online(input)\n",
    "      elif model == 'target':\n",
    "          return self.target(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97226837",
   "metadata": {},
   "source": [
    "TD ESTIMATE & TD TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb211864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.gamma = 0.9\n",
    "\n",
    "  def td_estimate(self, state, action):\n",
    "    current_Q = self.net(state, model='online')[np.arange(0, self.batch_size), action] # Q_online(s,a)\n",
    "    return current_Q\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def td_target(self, reward, next_state, done):\n",
    "    next_state_Q = self.net(next_state, model='online')\n",
    "    best_action = torch.argmax(next_state_Q, axis=1)\n",
    "    next_Q = self.net(next_state, model='target')[np.arange(0, self.batch_size), best_action]\n",
    "    return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619fc32",
   "metadata": {},
   "source": [
    "UPDATING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c088fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "      super().__init__(state_dim, action_dim, save_dir)\n",
    "      self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "      self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target) :\n",
    "      loss = self.loss_fn(td_estimate, td_target)\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "      return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "      self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab04e5f",
   "metadata": {},
   "source": [
    "SAVE CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bd8e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=self.net.state_dict(),\n",
    "                exploration_rate=self.exploration_rate\n",
    "            ),\n",
    "            save_path\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae56fac8",
   "metadata": {},
   "source": [
    "ALL TGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53c7dbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e5  # min. experiences before training\n",
    "        self.learn_every = 6   # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4   # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "      if self.curr_step % self.sync_every == 0:\n",
    "          self.sync_Q_target()\n",
    "\n",
    "      if self.curr_step % self.save_every == 0:\n",
    "          self.save()\n",
    "\n",
    "      if self.curr_step < self.burnin:\n",
    "          return None, None\n",
    "\n",
    "      if self.curr_step % self.learn_every != 0:\n",
    "          return None, None\n",
    "\n",
    "      # Sample from memory\n",
    "      state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "      # Get TD Estimate\n",
    "      td_est = self.td_estimate(state, action)\n",
    "\n",
    "      # Get TD Target\n",
    "      td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "      # Backpropagate loss through Q_online\n",
    "      loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "      return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83970934",
   "metadata": {},
   "source": [
    "LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f88381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MetricLogger():\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b94581",
   "metadata": {},
   "source": [
    "PLAY MARIO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86be90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: DeprecationWarning: invalid escape sequence \\W\n",
      "C:\\Users\\KONTO\\AppData\\Local\\Temp\\ipykernel_17296\\3217023846.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\PYTHON39\\lib\\site-packages\\gym_super_mario_bros\\smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n",
      "C:\\Users\\KONTO\\AppData\\Local\\Temp\\ipykernel_17296\\3217023846.py:22: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 346 - Epsilon 0.9999135037301933 - Mean Reward 707.0 - Mean Length 346.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 4.613 - Time 2022-05-26T14:09:44\n",
      "Episode 20 - Step 8681 - Epsilon 0.9978321030188155 - Mean Reward 715.81 - Mean Length 413.381 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 112.676 - Time 2022-05-26T14:11:37\n",
      "Episode 40 - Step 16652 - Epsilon 0.9958456527536822 - Mean Reward 745.756 - Mean Length 406.146 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 101.388 - Time 2022-05-26T14:13:18\n",
      "Episode 60 - Step 24430 - Epsilon 0.9939111121065121 - Mean Reward 737.082 - Mean Length 400.492 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 97.665 - Time 2022-05-26T14:14:56\n",
      "Episode 80 - Step 30878 - Epsilon 0.9923102178608908 - Mean Reward 731.111 - Mean Length 381.21 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 82.88 - Time 2022-05-26T14:16:19\n",
      "Episode 100 - Step 36415 - Epsilon 0.9909375625382856 - Mean Reward 723.96 - Mean Length 360.69 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 75.413 - Time 2022-05-26T14:17:34\n",
      "Episode 120 - Step 45696 - Epsilon 0.9886410046958837 - Mean Reward 738.89 - Mean Length 370.15 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 124.68 - Time 2022-05-26T14:19:39\n",
      "Episode 140 - Step 53024 - Epsilon 0.9868314721867173 - Mean Reward 757.61 - Mean Length 363.72 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 99.175 - Time 2022-05-26T14:21:18\n",
      "Episode 160 - Step 58921 - Epsilon 0.9853777075764613 - Mean Reward 743.24 - Mean Length 344.91 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 79.786 - Time 2022-05-26T14:22:38\n",
      "Episode 180 - Step 65540 - Epsilon 0.9837485019467392 - Mean Reward 748.89 - Mean Length 346.62 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 90.034 - Time 2022-05-26T14:24:08\n",
      "Episode 200 - Step 72597 - Epsilon 0.9820144535324214 - Mean Reward 761.12 - Mean Length 361.82 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 95.196 - Time 2022-05-26T14:25:43\n",
      "Episode 220 - Step 78907 - Epsilon 0.9804665467707261 - Mean Reward 746.5 - Mean Length 332.11 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 81.738 - Time 2022-05-26T14:27:05\n",
      "Episode 240 - Step 86225 - Epsilon 0.9786744228443698 - Mean Reward 710.57 - Mean Length 332.01 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 91.904 - Time 2022-05-26T14:28:37\n",
      "Episode 260 - Step 93309 - Epsilon 0.9769427240966659 - Mean Reward 724.24 - Mean Length 343.88 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 89.558 - Time 2022-05-26T14:30:06\n",
      "Episode 280 - Step 100800 - Epsilon 0.9751148664757774 - Mean Reward 723.36 - Mean Length 352.6 - Mean Loss 0.019 - Mean Q Value 0.008 - Time Delta 103.06 - Time 2022-05-26T14:31:49\n",
      "Episode 300 - Step 105869 - Epsilon 0.973879934655639 - Mean Reward 694.54 - Mean Length 332.72 - Mean Loss 0.098 - Mean Q Value 0.426 - Time Delta 87.784 - Time 2022-05-26T14:33:17\n",
      "Episode 320 - Step 113331 - Epsilon 0.9720648549523806 - Mean Reward 699.82 - Mean Length 344.24 - Mean Loss 0.174 - Mean Q Value 1.075 - Time Delta 129.258 - Time 2022-05-26T14:35:26\n",
      "Episode 340 - Step 118637 - Epsilon 0.9707762656068689 - Mean Reward 683.38 - Mean Length 324.12 - Mean Loss 0.241 - Mean Q Value 1.951 - Time Delta 91.24 - Time 2022-05-26T14:36:58\n",
      "Episode 360 - Step 125460 - Epsilon 0.9691217752593752 - Mean Reward 689.22 - Mean Length 321.51 - Mean Loss 0.328 - Mean Q Value 3.154 - Time Delta 117.607 - Time 2022-05-26T14:38:55\n",
      "Episode 380 - Step 131890 - Epsilon 0.9675651632729334 - Mean Reward 661.11 - Mean Length 310.9 - Mean Loss 0.381 - Mean Q Value 4.469 - Time Delta 108.75 - Time 2022-05-26T14:40:44\n",
      "Episode 400 - Step 138368 - Epsilon 0.9659994594659441 - Mean Reward 673.72 - Mean Length 324.99 - Mean Loss 0.376 - Mean Q Value 5.391 - Time Delta 107.349 - Time 2022-05-26T14:42:31\n",
      "Episode 420 - Step 145107 - Epsilon 0.9643733618417284 - Mean Reward 657.24 - Mean Length 317.76 - Mean Loss 0.39 - Mean Q Value 6.413 - Time Delta 111.979 - Time 2022-05-26T14:44:23\n",
      "Episode 440 - Step 152381 - Epsilon 0.962621242261534 - Mean Reward 662.01 - Mean Length 337.44 - Mean Loss 0.421 - Mean Q Value 7.265 - Time Delta 125.692 - Time 2022-05-26T14:46:29\n",
      "Episode 460 - Step 158689 - Epsilon 0.9611043847291919 - Mean Reward 650.91 - Mean Length 332.29 - Mean Loss 0.418 - Mean Q Value 7.704 - Time Delta 110.311 - Time 2022-05-26T14:48:19\n",
      "Episode 480 - Step 165574 - Epsilon 0.9594515065165303 - Mean Reward 662.58 - Mean Length 336.84 - Mean Loss 0.463 - Mean Q Value 8.513 - Time Delta 22297.927 - Time 2022-05-26T20:59:57\n",
      "Episode 500 - Step 171929 - Epsilon 0.9579283879926009 - Mean Reward 665.76 - Mean Length 335.61 - Mean Loss 0.508 - Mean Q Value 9.413 - Time Delta 113.189 - Time 2022-05-26T21:01:50\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path('E:\\WYNIKI\\checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40000\n",
    "\n",
    "### for Loop that train the model num_episodes times by playing the game\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info['flag_get']:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=mario.exploration_rate,\n",
    "            step=mario.curr_step\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
